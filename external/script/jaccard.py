import json
import re
import unicodedata
from pathlib import Path
from tqdm import tqdm
from ckip_transformers.nlp import CkipWordSegmenter
import cupy as cp  # GPU acceleration with CuPy

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/zicai/origin_text.json")
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text/")
OUTPUT_JSON_PATH = Path(r"D:/zicai/sentence_allusion.json")
# åŒ…å«åŠå½¢ç©ºæ ¼
CHARS_TO_REMOVE = "ï¹”ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\]\\\\/ ,.:;!?~1234567890Â¶"
JACCARD_THRESHOLD = 0.7
BATCH_SIZE = 8192  # å¯èª¿æ•´æ‰¹æ¬¡å¤§å°

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [ # åŸå¥è‹¥ä»¥ä¸‹åˆ—è©å½™èµ·å§‹å‰‡åˆªé™¤è©²è©å½™
    "æ•…", "è§€å¤«", "å¤«å¦‚æ˜¯", "åŒ äºº", "å¾’è§€å…¶", "å¥è‹¥", "æ˜”", "å‰‡çŸ¥", "æ–¼æ˜¯", "å‡¡æ•™", "æ•…ç‹è€…", "å‰å¤«", "è‡ªç„¶", 
    "çŒ—å—Ÿ", "å‰‡çŸ¥", "å…¶æœ‰", "è±ˆæ¯”", 
]
SUFFIX_EXCLUDE = [ # åŸå¥è‹¥ä»¥ä¸‹åˆ—è©å½™çµå°¾å‰‡åˆªé™¤è©²è©å½™
  "è€…å“‰"
]

# ======== å·¥å…·å‡½å¼ ========
def clean_sentence(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            return text[len(prefix):].strip()
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            return text[:-len(suffix)].strip()
    return text.strip()

def normalize(text: str) -> str:
    return unicodedata.normalize("NFKC", text)

# ======== è®€å–èˆ‡æ¸…æ´—åŸå¥è³‡æ–™ ========
def load_parsed_results(path):
    print("ğŸ”„ è¼‰å…¥åŸå¥è³‡æ–™...")
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    records = []
    for art in data:
        for para in art.get("æ®µè½", []):
            for grp in para.get("å¥çµ„", []):
                for sent in grp.get("å¥å­", []):
                    orig = normalize(sent.get("å…§å®¹", ""))
                    fm = clean_sentence(orig)
                    if fm:
                        records.append({
                            "article_num":   art.get("ç¯‡è™Ÿ"),
                            "author":        art.get("è³¦å®¶"),
                            "article_title": art.get("è³¦ç¯‡"),
                            "paragraph_num": para.get("æ®µè½ç·¨è™Ÿ"),
                            "group_num":     grp.get("å¥çµ„ç·¨è™Ÿ"),
                            "sentence_num":  sent.get("å¥ç·¨è™Ÿ"),
                            "original":      orig,
                            "for_matching":  fm
                        })
    print(f"âœ… åŸå¥è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} æ¢ã€‚")
    return records

# ======== è®€å–èˆ‡æ¸…æ´—å¾…æ¯”å°å¥è³‡æ–™ ========
def load_compared_sentences(folder, chars):
    print("ğŸ”„ è¼‰å…¥å¾…æ¯”å°å¥è³‡æ–™...")
    pattern = f"[{re.escape(chars)}]"
    sents = []
    for fp in folder.rglob("*.txt"):
        raw = normalize(fp.read_text(encoding="utf-8").replace("\n", ""))
        raw = re.sub(r"<[^>]*>", "", raw)
        for idx, seg in enumerate(re.split(pattern, raw)):
            seg = normalize(seg)
            fm = clean_sentence(seg)
            if fm:
                sents.append({
                    "matched_file":  fp.parent.name + "/" + fp.stem,
                    "matched_index": idx,
                    "raw":           seg,
                    "for_matching":  fm
                })
    print(f"âœ… å¾…æ¯”å°è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢ã€‚")
    return sents

# ======== åˆ†è© ========
def segment_in_batches(sentences, segmenter, batch_size=500, text_type=""):
    print(f"ğŸªš é–‹å§‹åˆ†è©ï¼š{text_type}ï¼ˆå…± {len(sentences)} æ¢ï¼‰...")
    tokens = []
    with tqdm(total=len(sentences), desc=f"åˆ†è© ({text_type})", unit="å¥") as pbar:
        for i in range(0, len(sentences), batch_size):
            batch = [s["for_matching"] for s in sentences[i:i+batch_size]]
            toks = segmenter(batch, show_progress=False)
            tokens.extend(toks)
            pbar.update(len(batch))
    print(f"âœ… åˆ†è© ({text_type}) å®Œæˆï¼Œå…± {len(tokens)} æ¢ tokensã€‚")
    return tokens

# ======== è©å½™è¡¨å»ºç«‹ ========
def build_vocab(tokens_list):
    print("â¡ï¸ å»ºæ§‹è©å½™è¡¨...")
    vocab = sorted({w for toks in tokens_list for w in toks})
    print(f"âœ… è©å½™è¡¨å¤§å°ï¼š{len(vocab)} å€‹è©ã€‚")
    return {w: i for i, w in enumerate(vocab)}

# ======== å‘é‡åŒ– ========
def tokens_to_gpu_matrix(tokens_list, word2idx):
    n, d = len(tokens_list), len(word2idx)
    mat = cp.zeros((n, d), dtype=cp.int8)
    for i, toks in enumerate(tokens_list):
        for w in toks:
            idx = word2idx.get(w)
            if idx is not None:
                mat[i, idx] = 1
    return mat

# ======== æ‰¹æ¬¡ Jaccard è¨ˆç®— ========
def batch_jaccard_gpu(comp_mat, origin_mat):
    comp_f = comp_mat.astype(cp.float16)
    orig_f = origin_mat.astype(cp.float16)
    inter = comp_f.dot(orig_f.T)
    sc = comp_f.sum(axis=1, keepdims=True)
    so = orig_f.sum(axis=1, keepdims=True).T
    jac = inter / (sc + so - inter + 1e-9)
    return jac.max(axis=1), jac.argmax(axis=1)

# ======== ä¸»ç¨‹å¼ ==========
def main():
    origin   = load_parsed_results(PARSED_RESULTS_PATH)
    compared = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)

    ws = CkipWordSegmenter(device=0, model="bert-base")
    origin_tokens   = segment_in_batches(origin,   ws, batch_size=500, text_type="åŸå§‹æ–‡æœ¬")
    compared_tokens = segment_in_batches(compared, ws, batch_size=500, text_type="æ¯”å°æ–‡æœ¬")

    word2idx   = build_vocab(origin_tokens)
    origin_mat = tokens_to_gpu_matrix(origin_tokens, word2idx)

    total = len(compared)
    print(f"ğŸ§ª é–‹å§‹ Jaccard åŒ¹é…ï¼Œå…± {total} ç­†å¾…æ¯”å°ï¼ˆæ‰¹æ¬¡å¤§å° {BATCH_SIZE}ï¼‰ã€‚")

    matches = []
    # å–®ä¸€ tqdm é€²åº¦æ¢ï¼Œæ¯è™•ç†ä¸€å¥æ›´æ–°ä¸€æ¬¡
    with tqdm(total=total, desc="Jaccard åŒ¹é…", unit="å¥") as pbar:
        for st in range(0, total, BATCH_SIZE):
            ed = min(st + BATCH_SIZE, total)
            comp_batch_mat = tokens_to_gpu_matrix(compared_tokens[st:ed], word2idx)
            scores, idxs   = batch_jaccard_gpu(comp_batch_mat, origin_mat)
            scores, idxs   = cp.asnumpy(scores), cp.asnumpy(idxs)
            for i, (s, oid) in enumerate(zip(scores, idxs)):
                if s >= JACCARD_THRESHOLD:
                    od = origin[oid]
                    cd = compared[st + i]
                    matches.append({
                        **{k: od[k] for k in [
                            "article_num","author","article_title",
                            "paragraph_num","group_num","sentence_num","original"
                        ]},
                        "matched_file":   cd["matched_file"],
                        "matched_index":  cd["matched_index"],
                        "matched":        cd["raw"],
                        "similarity":     float(s)
                    })
                pbar.update(1)

    print(f"âœ… Jaccard åŒ¹é…å®Œæˆï¼Œå…±æ‰¾åˆ° {len(matches)} ç­†çµæœã€‚")
    print("ğŸ“„ æ’åºä¸¦è¼¸å‡º JSON æª”æ¡ˆ...")
    matches = sorted(
        matches,
        key=lambda x: (
            int(x["article_num"]),
            int(x["paragraph_num"]),
            int(x["group_num"]),
            int(x["sentence_num"])
        )
    )
    with OUTPUT_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(matches, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()