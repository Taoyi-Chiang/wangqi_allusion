import json
import re
import unicodedata
from pathlib import Path
from tqdm import tqdm
from ckip_transformers.nlp import CkipWordSegmenter
import cupy as cp  # GPU acceleration with CuPy

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/wangqi_allusion/output/origin_text.json")
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text/åä¸‰ç¶“")
OUTPUT_JSON_PATH = Path(r"D:/wangqi_allusion/output/sentence_allusion.json")
# åŒ…å«åŠå½¢ç©ºæ ¼
CHARS_TO_REMOVE = "ï¹”ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\]\\\\/ ,.:;!?~1234567890Â¶"
JACCARD_THRESHOLD = 0.7
BATCH_SIZE = 8192  # å¯èª¿æ•´æ‰¹æ¬¡å¤§å°

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶", "çŸå¤«", "çŸä¹ƒ", "è‡³å¤«", "æ‡¿å¤«", "è“‹ç”±æˆ‘å›", "é‡æ›°", "æ˜¯çŸ¥", "å—Ÿå¤«", "å¤«å…¶", "æ‡¿å…¶", "æ‰€ä»¥",
    "æƒ³å¤«", "å…¶å§‹ä¹Ÿ", "ç•¶å…¶", "æ³å¾©", "æ™‚å‰‡", "è‡³è‹¥", "è±ˆç¨", "è‹¥ä¹ƒ", "ä»Šå‰‡", "ä¹ƒçŸ¥", "æ—¢è€Œ", "å—Ÿä¹",
    "æ•…æˆ‘å", "è§€å¤«", "ç„¶è€Œ", "çˆ¾ä¹ƒ", "æ˜¯ä»¥", "åŸå¤«", "æ›·è‹¥", "æ–¯å‰‡", "æ–¼æ™‚", "æ–¹ä»Š", "äº¦ä½•å¿…", "è‹¥ç„¶",
    "å®¢æœ‰", "è‡³æ–¼", "å‰‡çŸ¥", "ä¸”å¤«", "æ–¯ä¹ƒ", "æ³", "æ–¼æ˜¯", "è¦©å¤«", "ä¸”å½¼", "è±ˆè‹¥", "å·²è€Œ", "å§‹ä¹Ÿ", "æ•…",
    "ç„¶å‰‡", "è±ˆå¦‚æˆ‘", "è±ˆä¸ä»¥", "æˆ‘åœ‹å®¶", "å…¶å·¥è€…", "æ‰€è¬‚", "ä»Šå¾å›", "åŠå¤«", "çˆ¾å…¶", "å°‡ä»¥", "å¯ä»¥", "ä»Š",
    "åœ‹å®¶", "ç„¶å¾Œ", "å‘éæˆ‘å", "å‰‡æœ‰", "å½¼", "æƒœä¹", "ç”±æ˜¯", "ä¹ƒè¨€æ›°", "è‹¥å¤«", "äº¦ä½•ç”¨", "ä¸ç„¶",
    "å˜‰å…¶", "ä»Šå‰‡", "å¾’ç¾å¤«", "æ•…èƒ½", "æœ‰æ¢è€…æ›°", "æƒœå¦‚", "è€Œæ³", "é€®å¤«", "èª å¤«", "æ–¼æˆ²", "æ´ä¹", "ä¼Šæ˜”",
    "å‰‡å°‡", "ä»Šå‰‡", "æ³ä»Š", "å£«æœ‰", "æš¨ä¹", "äº¦ä½•è¾¨å¤«", "ä¿¾å¤«", "äº¦çŒ¶", "ç»å¤«", "æ™‚ä¹Ÿ", "å›ºçŸ¥", "è¶³ä»¥",
    "çŸåœ‹å®¶", "æ¯”ä¹", "äº¦ç”±", "è§€å…¶", "å°‡ä¿¾ä¹", "è–äºº", "å›å­", "æ–¼ä»¥", "ä¹ƒ", "æ–¯è“‹", "å™«", "å¤«æƒŸ",
    "é«˜çš‡å¸", "å¸æ—¢", "å˜‰å…¶", "å§‹å‰‡", "åˆå®‰å¾—", "å…¶", "å„’æœ‰", "ç•¶æ˜¯æ™‚ä¹Ÿ", "å¤«ç„¶", "å®œä¹", "æ•…å…¶", "åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ", "ä»Šæˆ‘åœ‹å®¶", "æ˜¯æ™‚", "æœ‰å¸", "å‘è‹¥", "æˆ‘çš‡", "æ•…ç‹è€…", "å‰‡", "é„’å­", "å­°", "æš¨å¤«", "ç”¨èƒ½",
    "æ•…å°‡", "æ³å…¶", "æ•…å®œ", "ç‹è€…", "è–ä¸Š", "å…ˆç‹", "ä¹ƒæœ‰", "æ³ä¹ƒ", "åˆ¥æœ‰", "ä»Šè€…", "å›ºå®œ", "çš‡ä¸Š", "ä¸”å…¶",
    "å¾’è§€å¤«", "å¸å ¯ä»¥", "å§‹å…¶", "å€è€Œ", "ä¹ƒæ›°", "å‘ä½¿", "æ¼¢æ­¦å¸", "å…ˆæ˜¯", "ä»–æ—¥", "ä¹ƒå‘½", "è§€ä¹", "åœ‹å®¶ä»¥",
    "å¢¨å­", "å€Ÿå¦‚", "è¶³ä»¥", "ä¸Šä¹ƒ", "å—šå‘¼", "æ˜”ä¼Š", "å…ˆè³¢", "é‚ä½¿", "è±ˆæ¯”å¤«", "å›ºå…¶", "æ³æœ‰", "é­¯æ­ç‹", "çš‡å®¶",
    "å¾å›æ˜¯æ™‚", "çŸ¥", "å‘¨ç©†ç‹", "å‰‡æœ‰", "æ˜¯ç”¨", "ä¹ƒè¨€æ›°", "åŠ", "æ•…å¤«", "çŸä¹", "å¤«ä»¥", "å¯§ä»¤", "å¦‚", "ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ", "é‚", "æ‚²å¤«", "å®‰å¾—", "æ•…å¾—", "ä¸”è¦‹å…¶", "æ˜¯ä½•", "è«ä¸", "å£«æœ‰", "çŸ¥å…¶", "æœªè‹¥", "è“‹ä»¥", "å›ºå¯ä»¥",
    "è±ˆå¾’", "ï¤€æ¯”å¤«", "æ˜¯æ•…"
]
SUFFIX_EXCLUDE = [
    "æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰", "ä¹", "ç„‰", "è€…ä¹Ÿ", "ä¹ŸçŸ£å“‰"
]

# ======== å·¥å…·å‡½å¼ ========
def clean_sentence(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            return text[len(prefix):].strip()
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            return text[:-len(suffix)].strip()
    return text.strip()

def normalize(text: str) -> str:
    return unicodedata.normalize("NFKC", text)

# ======== è®€å–èˆ‡æ¸…æ´—åŸå¥è³‡æ–™ ========
def load_parsed_results(path):
    print("ğŸ”„ è¼‰å…¥åŸå¥è³‡æ–™...")
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    records = []
    for art in data:
        for para in art.get("æ®µè½", []):
            for grp in para.get("å¥çµ„", []):
                for sent in grp.get("å¥å­", []):
                    orig = normalize(sent.get("å…§å®¹", ""))
                    fm = clean_sentence(orig)
                    if fm:
                        records.append({
                            "article_num":   art.get("ç¯‡è™Ÿ"),
                            "author":        art.get("è³¦å®¶"),
                            "article_title": art.get("è³¦ç¯‡"),
                            "paragraph_num": para.get("æ®µè½ç·¨è™Ÿ"),
                            "group_num":     grp.get("å¥çµ„ç·¨è™Ÿ"),
                            "sentence_num":  sent.get("å¥ç·¨è™Ÿ"),
                            "original":      orig,
                            "for_matching":  fm
                        })
    print(f"âœ… åŸå¥è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} æ¢ã€‚")
    return records

# ======== è®€å–èˆ‡æ¸…æ´—å¾…æ¯”å°å¥è³‡æ–™ ========
def load_compared_sentences(folder, chars):
    print("ğŸ”„ è¼‰å…¥å¾…æ¯”å°å¥è³‡æ–™...")
    pattern = f"[{re.escape(chars)}]"
    sents = []
    for fp in folder.rglob("*.txt"):
        raw = normalize(fp.read_text(encoding="utf-8").replace("\n", ""))
        raw = re.sub(r"<[^>]*>", "", raw)
        for idx, seg in enumerate(re.split(pattern, raw)):
            seg = normalize(seg)
            fm = clean_sentence(seg)
            if fm:
                sents.append({
                    "matched_file":  fp.parent.name + "/" + fp.stem,
                    "matched_index": idx,
                    "raw":           seg,
                    "for_matching":  fm
                })
    print(f"âœ… å¾…æ¯”å°è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢ã€‚")
    return sents

# ======== åˆ†è© ========
def segment_in_batches(sentences, segmenter, batch_size=500, text_type=""):
    print(f"ğŸªš é–‹å§‹åˆ†è©ï¼š{text_type}ï¼ˆå…± {len(sentences)} æ¢ï¼‰...")
    tokens = []
    with tqdm(total=len(sentences), desc=f"åˆ†è© ({text_type})", unit="å¥") as pbar:
        for i in range(0, len(sentences), batch_size):
            batch = [s["for_matching"] for s in sentences[i:i+batch_size]]
            toks = segmenter(batch, show_progress=False)
            tokens.extend(toks)
            pbar.update(len(batch))
    print(f"âœ… åˆ†è© ({text_type}) å®Œæˆï¼Œå…± {len(tokens)} æ¢ tokensã€‚")
    return tokens

# ======== è©å½™è¡¨å»ºç«‹ ========
def build_vocab(tokens_list):
    print("â¡ï¸ å»ºæ§‹è©å½™è¡¨...")
    vocab = sorted({w for toks in tokens_list for w in toks})
    print(f"âœ… è©å½™è¡¨å¤§å°ï¼š{len(vocab)} å€‹è©ã€‚")
    return {w: i for i, w in enumerate(vocab)}

# ======== å‘é‡åŒ– ========
def tokens_to_gpu_matrix(tokens_list, word2idx):
    n, d = len(tokens_list), len(word2idx)
    mat = cp.zeros((n, d), dtype=cp.int8)
    for i, toks in enumerate(tokens_list):
        for w in toks:
            idx = word2idx.get(w)
            if idx is not None:
                mat[i, idx] = 1
    return mat

# ======== æ‰¹æ¬¡ Jaccard è¨ˆç®— ========
def batch_jaccard_gpu(comp_mat, origin_mat):
    comp_f = comp_mat.astype(cp.float16)
    orig_f = origin_mat.astype(cp.float16)
    inter = comp_f.dot(orig_f.T)
    sc = comp_f.sum(axis=1, keepdims=True)
    so = orig_f.sum(axis=1, keepdims=True).T
    jac = inter / (sc + so - inter + 1e-9)
    return jac.max(axis=1), jac.argmax(axis=1)

# ======== ä¸»ç¨‹å¼ ==========
def main():
    origin   = load_parsed_results(PARSED_RESULTS_PATH)
    compared = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)

    ws = CkipWordSegmenter(device=0, model="bert-base")
    origin_tokens   = segment_in_batches(origin,   ws, batch_size=500, text_type="åŸå§‹æ–‡æœ¬")
    compared_tokens = segment_in_batches(compared, ws, batch_size=500, text_type="æ¯”å°æ–‡æœ¬")

    word2idx   = build_vocab(origin_tokens)
    origin_mat = tokens_to_gpu_matrix(origin_tokens, word2idx)

    total = len(compared)
    print(f"ğŸ§ª é–‹å§‹ Jaccard åŒ¹é…ï¼Œå…± {total} ç­†å¾…æ¯”å°ï¼ˆæ‰¹æ¬¡å¤§å° {BATCH_SIZE}ï¼‰ã€‚")

    matches = []
    # å–®ä¸€ tqdm é€²åº¦æ¢ï¼Œæ¯è™•ç†ä¸€å¥æ›´æ–°ä¸€æ¬¡
    with tqdm(total=total, desc="Jaccard åŒ¹é…", unit="å¥") as pbar:
        for st in range(0, total, BATCH_SIZE):
            ed = min(st + BATCH_SIZE, total)
            comp_batch_mat = tokens_to_gpu_matrix(compared_tokens[st:ed], word2idx)
            scores, idxs   = batch_jaccard_gpu(comp_batch_mat, origin_mat)
            scores, idxs   = cp.asnumpy(scores), cp.asnumpy(idxs)
            for i, (s, oid) in enumerate(zip(scores, idxs)):
                if s >= JACCARD_THRESHOLD:
                    od = origin[oid]
                    cd = compared[st + i]
                    matches.append({
                        **{k: od[k] for k in [
                            "article_num","author","article_title",
                            "paragraph_num","group_num","sentence_num","original"
                        ]},
                        "matched_file":   cd["matched_file"],
                        "matched_index":  cd["matched_index"],
                        "matched":        cd["raw"],
                        "similarity":     float(s)
                    })
                pbar.update(1)

    print(f"âœ… Jaccard åŒ¹é…å®Œæˆï¼Œå…±æ‰¾åˆ° {len(matches)} ç­†çµæœã€‚")
    print("ğŸ“„ æ’åºä¸¦è¼¸å‡º JSON æª”æ¡ˆ...")
    matches = sorted(
        matches,
        key=lambda x: (
            int(x["article_num"]),
            int(x["paragraph_num"]),
            int(x["group_num"]),
            int(x["sentence_num"])
        )
    )
    with OUTPUT_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(matches, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()