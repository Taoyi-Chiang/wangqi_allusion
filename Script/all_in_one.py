import csv
import json
import re
import unicodedata
from pathlib import Path
from tqdm import tqdm

# ============================================================================
# 1. è…³æœ¬é…ç½®å€ - æ‰€æœ‰åƒæ•¸éƒ½åœ¨é€™è£¡è¨­å®š
# ============================================================================
class Config:
    # è‹¥åªæƒ³è™•ç†ç‰¹å®šã€Œè³¦å®¶ã€ï¼Œè«‹å¡«å¯«åå­—ï¼›è‹¥æƒ³å…¨éƒ¨ä¸éæ¿¾ï¼Œå¯è¨­ç‚ºç©ºå­—ä¸² ""
    AUTHOR_NAME = "ç‹èµ·"

    # æª”æ¡ˆè·¯å¾‘è¨­å®š
    SENTENCE_MATCH_JSON_PATH = Path(r"D:/wangqi_allusion/output/origin_text.json")
    MAIN_JSON_PATH           = Path(r"D:/wangqi_allusion/output/manual_origin_text_ckip.json")
    COMPARED_TEXT_PATH       = Path(r"D:/lufu_allusion/data/raw/compared_text/åä¸‰ç¶“")
    OUTPUT_CSV_PATH          = Path(r"D:/wangqi_allusion/output/direct_allusion.csv")

    # æ¯”å°åƒæ•¸
    CHARS_TO_REMOVE = "ï¹”ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\]\\\\/ ,.:;!?~1234567890Â¶"
    NGRAM_RANGE     = [2, 3, 4]

    # æ’é™¤å­—ä¸²æ¸…å–® (è©ç´šæ¯”å°ç”¨)
    PREFIX_EXCLUDE = [
        "å¾’è§€å…¶", "çŸå¤«", "çŸä¹ƒ", "è‡³å¤«", "æ‡¿å¤«", "è“‹ç”±æˆ‘å›", "é‡æ›°", "æ˜¯çŸ¥", "å—Ÿå¤«", "å¤«å…¶", "æ‡¿å…¶", "æ‰€ä»¥",
        "æƒ³å¤«", "å…¶å§‹ä¹Ÿ", "ç•¶å…¶", "æ³å¾©", "æ™‚å‰‡", "è‡³è‹¥", "è±ˆç¨", "è‹¥ä¹ƒ", "ä»Šå‰‡", "ä¹ƒçŸ¥", "æ—¢è€Œ", "å—Ÿä¹",
        "æ•…æˆ‘å", "è§€å¤«", "ç„¶è€Œ", "çˆ¾ä¹ƒ", "æ˜¯ä»¥", "åŸå¤«", "æ›·è‹¥", "æ–¯å‰‡", "æ–¼æ™‚", "æ–¹ä»Š", "äº¦ä½•å¿…", "è‹¥ç„¶",
        "å®¢æœ‰", "è‡³æ–¼", "å‰‡çŸ¥", "ä¸”å¤«", "æ–¯ä¹ƒ", "æ³", "æ–¼æ˜¯", "è¦©å¤«", "ä¸”å½¼", "è±ˆè‹¥", "å·²è€Œ", "å§‹ä¹Ÿ", "æ•…",
        "ç„¶å‰‡", "è±ˆå¦‚æˆ‘", "è±ˆä¸ä»¥", "æˆ‘åœ‹å®¶", "å…¶å·¥è€…", "æ‰€è¬‚", "ä»Šå¾å›", "åŠå¤«", "çˆ¾å…¶", "å°‡ä»¥", "å¯ä»¥", "ä»Š",
        "åœ‹å®¶", "ç„¶å¾Œ", "å‘éæˆ‘å", "å‰‡æœ‰", "å½¼", "æƒœä¹", "ç”±æ˜¯", "ä¹ƒè¨€æ›°", "è‹¥å¤«", "äº¦ä½•ç”¨", "ä¸ç„¶",
        "å˜‰å…¶", "ä»Šå‰‡", "å¾’ç¾å¤«", "æ•…èƒ½", "æœ‰æ¢è€…æ›°", "æƒœå¦‚", "è€Œæ³", "é€®å¤«", "èª å¤«", "æ–¼æˆ²", "æ´ä¹", "ä¼Šæ˜”",
        "å‰‡å°‡", "ä»Šå‰‡", "æ³ä»Š", "å£«æœ‰", "æš¨ä¹", "äº¦ä½•è¾¨å¤«", "ä¿¾å¤«", "äº¦çŒ¶", "ç»å¤«", "æ™‚ä¹Ÿ", "å›ºçŸ¥", "è¶³ä»¥",
        "çŸåœ‹å®¶", "æ¯”ä¹", "äº¦ç”±", "è§€å…¶", "å°‡ä¿¾ä¹", "è–äºº", "å›å­", "æ–¼ä»¥", "ä¹ƒ", "æ–¯è“‹", "å™«", "å¤«æƒŸ",
        "é«˜çš‡å¸", "å¸æ—¢", "å˜‰å…¶", "å§‹å‰‡", "åˆå®‰å¾—", "å…¶", "å„’æœ‰", "ç•¶æ˜¯æ™‚ä¹Ÿ", "å¤«ç„¶", "å®œä¹", "æ•…å…¶", "åœ‹å®¶",
        "çˆ¾å…¶å§‹ä¹Ÿ", "ä»Šæˆ‘åœ‹å®¶", "æ˜¯æ™‚", "æœ‰å¸", "å‘è‹¥", "æˆ‘çš‡", "æ•…ç‹è€…", "å‰‡", "é„’å­", "å­°", "æš¨å¤«", "ç”¨èƒ½",
        "æ•…å°‡", "æ³å…¶", "æ•…å®œ", "ç‹è€…", "è–ä¸Š", "å…ˆç‹", "ä¹ƒæœ‰", "æ³ä¹ƒ", "åˆ¥æœ‰", "ä»Šè€…", "å›ºå®œ", "çš‡ä¸Š", "ä¸”å…¶",
        "å¾’è§€å¤«", "å¸å ¯ä»¥", "å§‹å…¶", "å€è€Œ", "ä¹ƒæ›°", "å‘ä½¿", "æ¼¢æ­¦å¸", "å…ˆæ˜¯", "ä»–æ—¥", "ä¹ƒå‘½", "è§€ä¹", "åœ‹å®¶ä»¥",
        "å¢¨å­", "å€Ÿå¦‚", "è¶³ä»¥", "ä¸Šä¹ƒ", "å—šå‘¼", "æ˜”ä¼Š", "å…ˆè³¢", "é‚ä½¿", "è±ˆæ¯”å¤«", "å›ºå…¶", "æ³æœ‰", "é­¯æ­ç‹", "çš‡å®¶",
        "å¾å›æ˜¯æ™‚", "çŸ¥", "å‘¨ç©†ç‹", "å‰‡æœ‰", "æ˜¯ç”¨", "ä¹ƒè¨€æ›°", "åŠ", "æ•…å¤«", "çŸä¹", "å¤«ä»¥", "å¯§ä»¤", "å¦‚", "ç„¶å‰‡",
        "æ»…æ˜ä¹ƒ", "é‚", "æ‚²å¤«", "å®‰å¾—", "æ•…å¾—", "ä¸”è¦‹å…¶", "æ˜¯ä½•", "è«ä¸", "å£«æœ‰", "çŸ¥å…¶", "æœªè‹¥", "è“‹ä»¥", "å›ºå¯ä»¥",
        "è±ˆå¾’", "è±ˆæ¯”å¤«", "æ˜¯æ•…"
    ]
    SUFFIX_EXCLUDE = [
        "æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰", "ä¹", "ç„‰", "è€…ä¹Ÿ", "ä¹ŸçŸ£å“‰"
    ]
    EXCLUDE_TOKENS = set(PREFIX_EXCLUDE + SUFFIX_EXCLUDE)

# ============================================================================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•¸
# ============================================================================
def normalize(text: str) -> str:
    """çµ±ä¸€åŒ–æ–‡å­—æ ¼å¼ã€‚"""
    return unicodedata.normalize("NFKC", text)

def extract_char_ngrams(chars: list, n: int) -> list:
    """å¾å­—å…ƒåˆ—è¡¨ä¸­æå– N-gramã€‚"""
    return ["".join(chars[i:i+n]) for i in range(len(chars) - n + 1)]

def load_and_process_data():
    """è¼‰å…¥ä¸»è³‡æ–™èˆ‡å¥ç´šæ¯”å°çµæœï¼Œä¸¦é å…ˆè™•ç†ã€‚"""
    print("ğŸš€ æ­£åœ¨è¼‰å…¥èˆ‡è™•ç†è³‡æ–™...")

    # è¼‰å…¥å¥ç´šæ¯”å°çµæœï¼Œå»ºç«‹å·²åŒ¹é…å¥å­çš„é›†åˆ
    try:
        sentence_match_list = json.load(open(Config.SENTENCE_MATCH_JSON_PATH, "r", encoding="utf-8"))
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {Config.SENTENCE_MATCH_JSON_PATH}ï¼Œå°‡è·³éå¥ç´šæ¯”å°ã€‚")
        sentence_match_list = []
    
    sentence_matched_keys = set()
    for entry in sentence_match_list:
        entry_author = entry.get("author", "").strip()
        if Config.AUTHOR_NAME and entry_author != Config.AUTHOR_NAME:
            continue
        key = (entry_author, entry.get("article_title", "").strip(), 
               str(entry.get("paragraph_num", "")).strip(), str(entry.get("group_num", "")).strip(), 
               str(entry.get("sentence_num", "")).strip())
        sentence_matched_keys.add(key)

    # è¼‰å…¥ä¸» JSON æª”æ¡ˆï¼Œå»ºç«‹æŸ¥æ‰¾å­—å…¸
    main_data = json.load(open(Config.MAIN_JSON_PATH, "r", encoding="utf-8"))
    main_lookup = {}
    sentence_order_map = {}
    hash_to_terms = {}
    
    order_counter = 0
    term_order = 0

    for essay in main_data:
        author = str(essay.get("è³¦å®¶", "")).strip()
        if Config.AUTHOR_NAME and author != Config.AUTHOR_NAME:
            continue
        article_title = str(essay.get("è³¦ç¯‡", "")).strip()
        art_no = str(essay.get("ç¯‡è™Ÿ", "")).strip()

        for para in essay.get("æ®µè½", []):
            para_no = str(para.get("æ®µè½ç·¨è™Ÿ", "")).strip()
            for grp in para.get("å¥çµ„", []):
                grp_no = str(grp.get("å¥çµ„ç·¨è™Ÿ", "")).strip()
                for sentence in grp.get("å¥å­", []):
                    sent_no = str(sentence.get("å¥ç·¨è™Ÿ", "")).strip()
                    # *** é‡è¦ä¿®æ­£: å¾ "å…§å®¹" æ¬„ä½è®€å–åŸå§‹å¥ ***
                    orig_sent = sentence.get("å…§å®¹", "").strip() 

                    key_sentence = (author, article_title, para_no, grp_no, sent_no)
                    main_lookup[key_sentence] = (art_no, orig_sent)
                    sentence_order_map[key_sentence] = order_counter
                    order_counter += 1

                    if key_sentence in sentence_matched_keys:
                        continue

                    token_list = sentence.get("tokens", [])
                    if not isinstance(token_list, list):
                        continue

                    for token in token_list:
                        tok = token.strip()
                        if not tok or tok in Config.EXCLUDE_TOKENS:
                            continue
                        
                        token_hash = hash(tok) & 0x7FFFFFFFFFFFFFFF
                        term_info = {
                            "order": term_order, "ç¯‡è™Ÿ": art_no, "è³¦å®¶": author, "è³¦ç¯‡": article_title,
                            "æ®µè½ç·¨è™Ÿ": para_no, "å¥çµ„ç·¨è™Ÿ": grp_no, "å¥ç·¨è™Ÿ": sent_no,
                            "åŸå§‹å¥": orig_sent, "tokens": tok
                        }
                        hash_to_terms.setdefault(token_hash, []).append(term_info)
                        term_order += 1

    all_term_hashes = set(hash_to_terms.keys())
    return sentence_match_list, sentence_matched_keys, main_lookup, sentence_order_map, hash_to_terms, all_term_hashes

def perform_token_matching(hash_to_terms, all_term_hashes, sentence_matched_keys, main_lookup, sentence_order_map):
    """åŸ·è¡Œè©ç´šæ¯”å°ï¼Œç”¢ç”Ÿçµæœã€‚"""
    print("ğŸ” æ­£åœ¨é€²è¡Œè©ç´šæ¯”å°...")
    token_level_matches = {}
    all_txt_files = list(Config.COMPARED_TEXT_PATH.rglob("*.txt"))

    for fp in tqdm(all_txt_files, desc="æƒææ¯”å°æª”æ¡ˆ", unit="file"):
        raw = normalize(fp.read_text(encoding='utf-8')).replace("\n", "")
        raw = re.sub(r"<[^>]*>", "", raw)
        segments = re.split(f"[{re.escape(Config.CHARS_TO_REMOVE)}]", raw)

        for idx, seg in enumerate(segments):
            seg = normalize(seg).strip()
            if not seg or len(seg) < min(Config.NGRAM_RANGE):
                continue
            
            seg_hashes = set()
            chars = list(seg)
            for n in Config.NGRAM_RANGE:
                for gram in extract_char_ngrams(chars, n):
                    g_hash = hash(gram) & 0x7FFFFFFFFFFFFFFF
                    if g_hash in all_term_hashes:
                        seg_hashes.add(g_hash)

            for term_hash in seg_hashes:
                for term_info in hash_to_terms[term_hash]:
                    key_sentence = (term_info["è³¦å®¶"], term_info["è³¦ç¯‡"], term_info["æ®µè½ç·¨è™Ÿ"],
                                    term_info["å¥çµ„ç·¨è™Ÿ"], term_info["å¥ç·¨è™Ÿ"])

                    if key_sentence in sentence_matched_keys:
                        continue
                    
                    art_no, orig_sentence = main_lookup.get(key_sentence, ("", ""))
                    
                    match_dict = {
                        "sentence_order": sentence_order_map.get(key_sentence, float('inf')),
                        "order": term_info["order"], "ç¯‡è™Ÿ": art_no, "è³¦å®¶": term_info["è³¦å®¶"],
                        "è³¦ç¯‡": term_info["è³¦ç¯‡"], "æ®µè½ç·¨è™Ÿ": term_info["æ®µè½ç·¨è™Ÿ"],
                        "å¥çµ„ç·¨è™Ÿ": term_info["å¥çµ„ç·¨è™Ÿ"], "å¥ç·¨è™Ÿ": term_info["å¥ç·¨è™Ÿ"],
                        "åŸå§‹å¥": orig_sentence, "tokens": term_info["tokens"],
                        "matched_file": f"{fp.parent.name}\\{fp.stem}",
                        "matched_index": idx, "matched": seg, "similarity": "NA"
                    }
                    token_level_matches.setdefault(key_sentence, []).append(match_dict)
    
    return token_level_matches

def generate_and_save_csv(matches):
    """å°‡æ‰€æœ‰æ¯”å°çµæœå¯«å…¥ CSV æª”æ¡ˆã€‚"""
    csv_headers = [
        "ç¯‡è™Ÿ", "è³¦å®¶", "è³¦ç¯‡", "æ®µè½ç·¨è™Ÿ", "å¥çµ„ç·¨è™Ÿ", "å¥ç·¨è™Ÿ", "åŸå§‹å¥", "tokens",
        "matched_file", "matched_index", "matched", "similarity"
    ]

    Config.OUTPUT_CSV_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(Config.OUTPUT_CSV_PATH, 'w', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=csv_headers)
        writer.writeheader()
        for m in matches:
            writer.writerow({key: m.get(key, "") for key in csv_headers})

    print(f"âœ… å®Œæˆï¼å…± {len(matches)} ç­†çµæœï¼Œå·²å­˜åˆ° {Config.OUTPUT_CSV_PATH}")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸ã€‚"""
    sentence_match_list, sentence_matched_keys, main_lookup, sentence_order_map, hash_to_terms, all_term_hashes = load_and_process_data()
    token_level_matches = perform_token_matching(hash_to_terms, all_term_hashes, sentence_matched_keys, main_lookup, sentence_order_map)

    # åˆä½µæ‰€æœ‰æ¯”å°çµæœ
    all_matches = []
    sorted_keys = sorted(sentence_order_map.keys(), key=lambda k: sentence_order_map[k])

    for key_sentence in sorted_keys:
        # å°‡è§£åŒ…çš„ç¨‹å¼ç¢¼ç§»åˆ°é€™è£¡ï¼Œåœ¨ä»»ä½• 'continue' èªå¥ä¹‹å‰
        author, article_title, para_no, grp_no, sent_no = key_sentence

        # ç¾åœ¨ç¯©é¸æ¢ä»¶å¯ä»¥æ”¾åœ¨é€™è£¡ï¼Œå› ç‚ºè®Šæ•¸å·²ç¶“è¢«å®šç¾©äº†
        if Config.AUTHOR_NAME and author != Config.AUTHOR_NAME:
            continue
        
        art_no, orig_sent = main_lookup.get(key_sentence, ("", ""))

        if key_sentence in sentence_matched_keys:
            for entry in sentence_match_list:
                k = (entry.get("author", "").strip(), entry.get("article_title", "").strip(), 
                     str(entry.get("paragraph_num", "")).strip(), str(entry.get("group_num", "")).strip(), 
                     str(entry.get("sentence_num", "")).strip())
                if k == key_sentence:
                    all_matches.append({
                        "ç¯‡è™Ÿ": str(entry.get("article_num", "")).strip(), "è³¦å®¶": entry.get("author", ""),
                        "è³¦ç¯‡": entry.get("article_title", ""), "æ®µè½ç·¨è™Ÿ": str(entry.get("paragraph_num", "")).strip(),
                        "å¥çµ„ç·¨è™Ÿ": str(entry.get("group_num", "")).strip(), "å¥ç·¨è™Ÿ": str(entry.get("sentence_num", "")).strip(),
                        "åŸå§‹å¥": entry.get("original", ""), "tokens": entry.get("original", ""),
                        "matched_file": entry.get("matched_file", ""), "matched_index": entry.get("matched_index", ""),
                        "matched": entry.get("matched", ""), "similarity": str(entry.get("similarity", ""))
                    })
            continue

        if key_sentence in token_level_matches:
            # åœ¨é€™è£¡ï¼Œæˆ‘å€‘å¾ main_lookup é‡æ–°ç²å–å®Œæ•´çš„åŸå§‹å¥ï¼Œè€Œä¸æ˜¯ä¾è³´ token_level_matches è£¡çš„èˆŠå€¼
            art_no, orig_sent_from_main = main_lookup.get(key_sentence, ("", ""))
            
            sorted_token_matches = sorted(token_level_matches[key_sentence], key=lambda x: x["order"])
            
            for ref in sorted_token_matches:
                all_matches.append({
                    "ç¯‡è™Ÿ": art_no, 
                    "è³¦å®¶": author, 
                    "è³¦ç¯‡": article_title,
                    "æ®µè½ç·¨è™Ÿ": para_no, 
                    "å¥çµ„ç·¨è™Ÿ": grp_no, 
                    "å¥ç·¨è™Ÿ": sent_no,
                    "åŸå§‹å¥": orig_sent_from_main,  # <-- ä½¿ç”¨å¾ main_lookup è®€å–åˆ°çš„å®Œæ•´åŸå§‹å¥
                    "tokens": ref["tokens"],
                    "matched_file": ref["matched_file"], 
                    "matched_index": ref["matched_index"],
                    "matched": ref["matched"], 
                    "similarity": ref["similarity"]
                })
            continue

        all_matches.append({
            "ç¯‡è™Ÿ": art_no, "è³¦å®¶": author, "è³¦ç¯‡": article_title, "æ®µè½ç·¨è™Ÿ": para_no,
            "å¥çµ„ç·¨è™Ÿ": grp_no, "å¥ç·¨è™Ÿ": sent_no, "åŸå§‹å¥": orig_sent,
            "tokens": "", "matched_file": "", "matched_index": "", "matched": "", "similarity": ""
        })

    generate_and_save_csv(all_matches)

if __name__ == "__main__":
    main()